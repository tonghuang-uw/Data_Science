# Deep Learning Model from scratch


Stochastic Gradient Descent (SGD) has long been the cornerstone of neural network optimization, due to its simplicity and effectiveness. However, the inherent challenges of training deep neural networks, such as slow convergence, susceptibility to local minima, and the difficulty of choosing an appropriate learning rate, have driven the development of advanced optimization techniques. Among these, SGD with Momentum, RMSprop, and Adam stand out as significant enhancements over plain SGD, each introducing unique mechanisms to address the challenges of deep learning optimization.
This project aims to conduct a comprehensive comparative analysis of these four optimization algorithms: SGD, SGD with Momentum, RMSprop, and Adam. Our comparison is grounded on several key metrics that capture aspects of optimization relevant to practitioners and researchers alike, including convergence speed, robustness to hyperparameter settings, and final model accuracy.

#### To pratice deep learning, I build all the model from scratch by only using numpy

## Objectives

This project evaluates the performance of these algorithms on a churn dataset, to understand their behavior and to analyze their efficiency in terms of convergence speed.

## Data

This project uses a churn data set on E commerce retail. There are 48 columns including different characteristics of users, and 49358 rows which represent the number of users in the dataset.

## Report
The complete report can be found here: [Deep Learning from Scratch](Deep_Learning/Deep_learning.pdf)




